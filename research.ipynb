{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas==1.5.3\n",
    "\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This version is depricated, so i used yt-dlp instead\n",
    "#import youtube_dl\n",
    "import yt_dlp\n",
    "import csv\n",
    "\n",
    "# loading \n",
    "from IPython.display import display\n",
    "# tables\n",
    "from tabulate import tabulate\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global playlistend\n",
    "global videosPerChannel\n",
    "global Query_Name\n",
    "\n",
    "playlistend = 50\n",
    "videosPerChannel = 50\n",
    "Query_Name = \"gamejam po polsku\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get search lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlTest=\"https://www.youtube.com/results?search_query=\"+Query_Name.replace(' ', '+')\n",
    "\n",
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts1 = {\n",
    "    'noabortonerror': True,\n",
    "    'ignoreerrors': True,\n",
    "    'skip_download': True,\n",
    "    'no_warnings': True,\n",
    "    'writesubtitles': False,\n",
    "    'writeautomaticsub': False,\n",
    "    'allsubtitles': False,\n",
    "    'listformats': False,\n",
    "    'forcejson': False,\n",
    "    'quiet': True,\n",
    "    'no_warnings': True,\n",
    "    'verbose': True,\n",
    "    'playlistend': playlistend\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts1) as ydl:\n",
    "    info =ydl.extract_info(urlTest, download=False)\n",
    "    for entry in info['entries']:\n",
    "        try:\n",
    "            url_list.append(entry['uploader_url'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            continue\n",
    "display(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding your youtube chanell to discover your views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_youtube_channel = \"https://www.youtube.com/@MroczneGranie\"\n",
    "url_list.append(my_youtube_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = list(set(url_list))\n",
    "display(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get content of channels (for training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeText(text):\n",
    "    return text.lower().replace(' ', '_').replace(',', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchChannel():\n",
    "    global videosPerChannel\n",
    "\n",
    "    ydl_opts = {\n",
    "        'noabortonerror': True,\n",
    "        'ignoreerrors': True,\n",
    "        'skip_download': True,\n",
    "        'no_warnings': True,\n",
    "        'writesubtitles': False,\n",
    "        'writeautomaticsub': False,\n",
    "        'allsubtitles': False,\n",
    "        'listformats': False,\n",
    "        'forcejson': False,\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "        'verbose': True,\n",
    "        'playlistend': videosPerChannel\n",
    "    }\n",
    "\n",
    "    video_data = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            for url in url_list:\n",
    "                info = ydl.extract_info(url+\"/videos\", download=False)\n",
    "                if info is not None and hasattr(info, 'get'):\n",
    "                    videos = info.get('entries')\n",
    "\n",
    "                    for video in videos:\n",
    "                        if video is not None and hasattr(info, 'get'):\n",
    "                            video_info = {\n",
    "                                'Channel': normalizeText(\"(uploader)\"+video.get('uploader', '')),\n",
    "                                'Title': video.get('title', '').lower(),\n",
    "                                'Views': video.get('view_count', ''),\n",
    "                                'Description': video.get('description', '').lower(),\n",
    "                                'Tags': normalizeText(','.join([\"(tag)\" + tag for tag in video.get('tags', [])])),\n",
    "                                'PhrasesTitle': \"\",\n",
    "                                'PhrasesDescription': \"\"\n",
    "                            }\n",
    "                            #display(video_info)\n",
    "                            video_data.append(video_info)\n",
    "                        \n",
    "                        backup = pd.DataFrame(video_data)\n",
    "                        backup.to_csv('backup.csv', index=False)\n",
    "            df = pd.DataFrame(video_data)  # Create a dataframe from the video data\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing URL: {url}')\n",
    "            print(e)\n",
    "            return pd.DataFrame()  # Return an empty dataframe if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelArray = FetchChannel()\n",
    "ChannelArray.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ChannelArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or import already Fetched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelArray = pd.read_csv('result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pharses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global numberOfPharses\n",
    "numberOfPharses = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_phrases(texts, saveDestination=\"phrases\", n=1000, max_phrase_length=4):\n",
    "    long_text = ' '.join(texts)\n",
    "    #long_text = long_text.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "    words = long_text.split()\n",
    "\n",
    "    phrases = []\n",
    "    for phrase_length in range(1, max_phrase_length + 1):\n",
    "        phrases += [' '.join(words[i:i+phrase_length]) for i in range(len(words)-phrase_length+1)]\n",
    "    \n",
    "    phrases = [s for s in phrases if len(s) >= 4]\n",
    "\n",
    "    phrase_counts = Counter(phrases)\n",
    "\n",
    "    with open(saveDestination+\".csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(phrase_counts.most_common(n))\n",
    "\n",
    "    return phrase_counts.most_common(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPharses(array,columnSource,columnDestination,prefix):\n",
    "    # Assuming ChannelArray is a DataFrame with a column named 'Title'\n",
    "    global numberOfPharses\n",
    "\n",
    "    titles = ChannelArray[columnSource].tolist()\n",
    "    common_phrases = find_most_common_phrases(titles,prefix,numberOfPharses)\n",
    "\n",
    "    if common_phrases:\n",
    "        common_phrases = [phrase for phrase, count in common_phrases]  # Extract only the phrases\n",
    "        \n",
    "        for index, row in ChannelArray.iterrows():\n",
    "            matching_elements = []\n",
    "            for common_phrase in common_phrases:\n",
    "                if common_phrase in row[columnSource]:\n",
    "                    matching_elements.append(prefix+common_phrase)\n",
    "            \n",
    "            #display(matching_elements)\n",
    "            ChannelArray.at[index, columnDestination] = normalizeText(','.join(matching_elements))\n",
    "            \n",
    "    else:\n",
    "        # Handle the case when common_phrases is empty\n",
    "        # For example, you can set the 'CommonPhrases' column to empty\n",
    "        ChannelArray[columnDestination] = \"\"\n",
    "\n",
    "findPharses(ChannelArray,'Title','PhrasesTitle',\"(title)\")\n",
    "findPharses(ChannelArray,'Description','PhrasesDescription',\"(description)\")\n",
    "\n",
    "ChannelArray.to_csv('result.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ChannelArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelArray['Channel'] = ChannelArray['Channel'].astype(str)\n",
    "ChannelArray['Title'] = ChannelArray['Title'].astype(str)\n",
    "ChannelArray['Views'] = ChannelArray['Views'].astype('int64')\n",
    "ChannelArray['Description'] = ChannelArray['Description'].astype(str)\n",
    "ChannelArray['Tags'] = ChannelArray['Tags'].astype(str)\n",
    "ChannelArray['PhrasesTitle'] = ChannelArray['PhrasesTitle'].astype(str)\n",
    "ChannelArray['PhrasesDescription'] = ChannelArray['PhrasesDescription'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ChannelArray.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling Raport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelArray.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR COLLECTING TAGS, DESCRIPTION, NAME AND CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging columns 1 5 6 and 7\n",
    "\n",
    "columns_to_merge = [0, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR COLLECTING TAGS AND CHANNEL ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_merge = [0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR COLLECTING TAGS AND CHANNEL AND TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_merge = [0, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ChannelArray.apply(lambda row: ' '.join([str(row[col]) for col in columns_to_merge]), axis=1)\n",
    "X = pd.DataFrame(X)\n",
    "X = X.rename(columns={0: 'keywords'})\n",
    "display(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets are just 2 column\n",
    "columns_to_merge = [2]\n",
    "Y = ChannelArray.apply(lambda row: ' '.join([str(row[col]) for col in columns_to_merge]), axis=1)\n",
    "Y = pd.DataFrame(Y)\n",
    "Y = Y.rename(columns={0: 'views'})\n",
    "Y['views'] = Y['views'].astype('int64')\n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keyword series list (list for columns in bool table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_series = pd.DataFrame(X['keywords'].str.split().explode().unique()).rename(columns={0: 'keywords'})\n",
    "keywords_series = X['keywords'].str.split().explode().unique()\n",
    "display(keywords_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating an empty DataFrame to store the transformed data\n",
    "X_transformed = pd.DataFrame(index=range(len(X)), columns=keywords_series)\n",
    "\n",
    "# Iterating over each element in X\n",
    "for row, keywords in X.iterrows():\n",
    "    for column in keywords_series:\n",
    "        if column in keywords['keywords']:\n",
    "            X_transformed.loc[int(row), str(column)] = True\n",
    "\n",
    "X_transformed = X_transformed.fillna(False)\n",
    "\n",
    "# Displaying the transformed DataFrame\n",
    "display(X_transformed)\n",
    "X_transformed.to_csv('pivotTable.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning value for tags (1 because channel not count for tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeleteColumnsWithOnly = 4 #True values\n",
    "deleteRowsWithOnly = 2 # Keywords\n",
    "OutlinerPrecise  = 2.0 # For Z-score outliner delete points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete rows with empty tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelArray = ChannelArray.dropna(subset=['Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete columns with only one instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of \"True\" values in each column\n",
    "counts = X_transformed.sum()\n",
    "\n",
    "# Get the column names that have less than two \"True\" values\n",
    "columns_to_delete = counts[counts < DeleteColumnsWithOnly].index\n",
    "\n",
    "# Delete the columns from the DataFrame\n",
    "X_transformed = X_transformed.drop(columns=columns_to_delete)\n",
    "\n",
    "display(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete rows with no tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of \"True\" values in each row\n",
    "counts = X_transformed.sum(axis=1)\n",
    "\n",
    "# Get the row indices that have less than two \"True\" values\n",
    "rows_to_delete = counts[counts < deleteRowsWithOnly].index\n",
    "\n",
    "# Delete the rows from the DataFrame\n",
    "X_transformed = X_transformed.drop(rows_to_delete)\n",
    "Y = Y.drop(rows_to_delete)\n",
    "\n",
    "# Transpose the DataFrame to reverse rows with columns\n",
    "#X_transformed = X_transformed.T\n",
    "\n",
    "display(X_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge X_transfered and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_XY = pd.concat([X_transformed, Y], axis=1)\n",
    "display(merged_XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Parameter for Outliner Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = sc.stats.zscore(Y)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_z_scores = (abs_z_scores < OutlinerPrecise).all(axis=1) #kind of mask\n",
    "NewMerged_XY = merged_XY[filtered_z_scores]\n",
    "NewMerged_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------- TAKE PARTS OF\n",
    "\n",
    "X_transformed = NewMerged_XY.iloc[:, :-1]\n",
    "Y = pd.DataFrame(NewMerged_XY.iloc[:, -1])\n",
    "\n",
    "#--------------------------------------------------------- REMOVE COLUMNS WITH ONLY\n",
    "\n",
    "# Count the number of \"True\" values in each column\n",
    "counts = X_transformed.sum()\n",
    "\n",
    "# Get the column names that have less than two \"True\" values\n",
    "columns_to_delete = counts[counts < DeleteColumnsWithOnly].index\n",
    "\n",
    "# Delete the columns from the DataFrame\n",
    "X_transformed = X_transformed.drop(columns=columns_to_delete)\n",
    "\n",
    "#--------------------------------------------------------- REMOVE ROWS WITH ONLY\n",
    "\n",
    "# Count the number of \"True\" values in each row\n",
    "counts = X_transformed.sum(axis=1)\n",
    "\n",
    "# Get the row indices that have less than two \"True\" values\n",
    "rows_to_delete = counts[counts < deleteRowsWithOnly].index\n",
    "\n",
    "# Delete the rows from the DataFrame\n",
    "X_transformed = X_transformed.drop(rows_to_delete)\n",
    "Y = Y.drop(rows_to_delete)\n",
    "\n",
    "# Transpose the DataFrame to reverse rows with columns\n",
    "#X_transformed = X_transformed.T\n",
    "\n",
    "#--------------------------------------------------------- MERGE AGAIN\n",
    "\n",
    "merged_XY = pd.concat([X_transformed, Y], axis=1)\n",
    "display(merged_XY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do. \n",
    "\n",
    "### Delete Keywords X with keywords connected with only one channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = NewMerged_XY.corr()\n",
    "target_column = 'views'\n",
    "correlations = correlation_matrix[target_column].drop(target_column)\n",
    "print(pd.DataFrame(correlations).sort_values(by=target_column, ascending=False).head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate model acc in LOOCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accept +- error in acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptErrorInAccuracy = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewMerged_XY.to_csv(\"TrainingData.csv\")\n",
    "Xval = NewMerged_XY.iloc[:, :-1]\n",
    "yval = NewMerged_XY.iloc[:, -1].values.ravel()\n",
    "\n",
    "# Initialize LOOCV\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Counter for predictions within the acceptable error range\n",
    "within_range_count = 0\n",
    "\n",
    "# Perform LOOCV\n",
    "for train_index, test_index in loo.split(Xval):\n",
    "    print(str(test_index[0])+\"/\"+str(len(Xval)))\n",
    "    X_train, X_test = Xval.values[train_index], Xval.values[test_index]\n",
    "    y_train, y_test = yval[train_index], yval[test_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)[0]\n",
    "\n",
    "    # Check if the prediction falls within the acceptable error range\n",
    "    if abs(y_pred - y_test[0]) <= acceptErrorInAccuracy:\n",
    "        within_range_count += 1\n",
    "\n",
    "# Calculate the percentage of predictions within the acceptable error range\n",
    "within_range_percentage = (within_range_count / len(yval)) * 100\n",
    "display(\"Percentage of predictions within the acceptable error range:\", within_range_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LogisticRegression()\n",
    "\n",
    "Percentage of predictions within the acceptable error range: 19.832985386221296\n",
    "\n",
    "acceptErrorInAccuracy = 25\n",
    "\n",
    "5 MIN 38 SEC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Accuracy: 0.006920415224913495 IN EXACT VALUE\n",
    "\n",
    "2 min 48 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficients (Thats whats we wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for logic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(Xval, yval)\n",
    "\n",
    "# Retrieve the coefficients from the trained model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame to store the variable names and coefficients\n",
    "coefficients_df = pd.DataFrame({'Variable': Xval.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# ---------------------------- COUNTING APPERIANCES ---------------------------------------------------------------\n",
    "# Count the number of occurrences of each variable in X\n",
    "variable_counts = Xval.astype(bool).sum(axis=0).reset_index()\n",
    "variable_counts.columns = ['Variable', 'Count']\n",
    "# Merge the variable counts with the coefficients DataFrame\n",
    "coefficients_df = pd.merge(coefficients_df, variable_counts, on='Variable')\n",
    "\n",
    "\n",
    "# Sort the coefficients by magnitude\n",
    "coefficients_df = coefficients_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Display the variables and their coefficients\n",
    "display(coefficients_df.head(25)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(Xval, yval)  # X is your feature matrix, y is the target vector\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "feature_names = Xval.columns\n",
    "\n",
    "# Get the indices of the top k features\n",
    "k = 25  # Set the number of top features you want to retrieve\n",
    "top_k_indices = np.argsort(feature_importances)[-k:]\n",
    "\n",
    "# Get the names of the top k features\n",
    "top_k_feature_names = [feature_names[i] for i in top_k_indices]\n",
    "\n",
    "display(top_k_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate HTML raport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn raport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ClassificationReport(model, classes=classes, support=True)\n",
    "report.fit(X_train, y_train)\n",
    "report.score(X_test, y_test)\n",
    "report.show(outpath=\"classification_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 2 min 4.9 sec for 213 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(NewMerged_XY)\n",
    "report.to_file(\"profile_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
